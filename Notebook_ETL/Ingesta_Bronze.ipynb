{"cells":[{"cell_type":"markdown","source":["# Copia de Datos de Clientes del Lakehouse Landing a Lakehouse Bronze"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"628cc4bb-038f-4e55-8f35-ca42a7756c1c"},{"cell_type":"code","source":["from pyspark.sql.functions import current_timestamp, sha2, concat_ws, col\n","from pyspark.sql.utils import AnalysisException\n","from delta.tables import DeltaTable\n","\n","# Configuración\n","src_catalog = \"Landing\"\n","dst_catalog = \"Bronze\"\n","tables_to_process = [\"Clientes\", \"Productos\", \"Tiendas\"]\n","\n","# Mapa de claves por tabla (ajusta según tu modelo)\n","key_map = {\n","    \"Clientes\": [\"ClienteID\"],\n","    \"Productos\": [\"ProductoID\"],\n","    \"Tiendas\": [\"TiendaID\"]\n","}\n","\n","# Helpers\n","def get_table_columns(table_full_name):\n","    try:\n","        schema = spark.table(table_full_name).schema\n","        return [(f.name, f.dataType.simpleString()) for f in schema]\n","    except AnalysisException:\n","        return []\n","\n","def add_ingestion_col_if_missing(table_full_name, dry_run=True):\n","    existing = get_table_columns(table_full_name)\n","    existing_names = [n for n, _ in existing]\n","    if \"ingestion_ts\" in existing_names:\n","        print(f\"[ALTER] ingestion_ts ya existe en {table_full_name}\")\n","        return True\n","    sql = f\"ALTER TABLE {table_full_name} ADD COLUMNS (ingestion_ts timestamp)\"\n","    if dry_run:\n","        print(f\"[ALTER dry_run] {sql}\")\n","        return True\n","    try:\n","        spark.sql(sql)\n","        print(f\"[ALTER] ingestion_ts agregado a {table_full_name}\")\n","        return True\n","    except Exception as e:\n","        print(f\"[ALTER] fallo al agregar ingestion_ts a {table_full_name}: {e}\")\n","        return False\n","\n","def build_diff_condition_sql(cols):\n","    \"\"\"\n","    Construye una condición SQL que detecta diferencias entre t.col y s.col,\n","    manejando NULLs correctamente.\n","    Devuelve una cadena con ORs entre comparaciones por columna.\n","    \"\"\"\n","    conds = []\n","    for c in cols:\n","        conds.append(\n","            f\"(t.`{c}` <> s.`{c}` OR (t.`{c}` IS NULL AND s.`{c}` IS NOT NULL) OR (t.`{c}` IS NOT NULL AND s.`{c}` IS NULL))\"\n","        )\n","    return \" OR \".join(conds)\n","\n","# Loop principal: MERGE condicional (actualiza solo si hay diferencias reales)\n","for tbl in tables_to_process:\n","    src_full = f\"{src_catalog}.{tbl}\"\n","    dst_full = f\"{dst_catalog}.{tbl}\"\n","    key_cols = key_map.get(tbl)\n","\n","    print(f\"\\n=== Procesando {src_full} -> {dst_full} (upsert condicional) ===\")\n","\n","    # Leer origen y añadir ingestion_ts (se actualizará solo si la fila cambia)\n","    try:\n","        df = spark.table(src_full).withColumn(\"ingestion_ts\", current_timestamp())\n","    except AnalysisException as e:\n","        print(f\"[ERROR] no se pudo leer {src_full}: {e}\")\n","        continue\n","\n","    if df.rdd.isEmpty():\n","        print(f\"[INFO] {src_full} está vacío. Omitiendo.\")\n","        continue\n","\n","    # Deduplicar por clave negocio en el DF (mantener la última fila por clave si hay duplicados)\n","    if key_cols:\n","        df = df.dropDuplicates(key_cols)\n","    else:\n","        print(f\"[WARN] No hay key_cols definidas para {tbl}; se procederá sin dedupe por clave.\")\n","\n","    # Asegurar ingestion_ts en la tabla destino (dry_run + aplicar)\n","    existing = get_table_columns(dst_full)\n","    if existing:\n","        add_ingestion_col_if_missing(dst_full, dry_run=True)\n","        add_ingestion_col_if_missing(dst_full, dry_run=False)\n","    else:\n","        print(f\"[INFO] {dst_full} no existe; se creará con el primer write.\")\n","\n","    # Columnas a comparar: todas menos ingestion_ts\n","    compare_cols = [c for c in df.columns if c != \"ingestion_ts\"]\n","    if not compare_cols:\n","        print(f\"[WARN] No hay columnas para comparar en {tbl} (solo ingestion_ts). Se hará MERGE normal.\")\n","    \n","    # Preparar conteos reales: n_total, n_new, n_changed (usando hashes)\n","    n_total = df.count()\n","    n_new = None\n","    n_changed = 0\n","    dst_exists = True if existing else False\n","\n","    if dst_exists and key_cols and compare_cols:\n","        try:\n","            dst_df = spark.table(dst_full).select(*key_cols, *compare_cols)\n","            # hashes para origen y destino (excluyendo ingestion_ts)\n","            df_h = df.select(*key_cols, *compare_cols).withColumn(\"_row_hash\", sha2(concat_ws(\"||\", *compare_cols), 256))\n","            dst_h = dst_df.withColumn(\"_row_hash\", sha2(concat_ws(\"||\", *compare_cols), 256)).select(*key_cols, \"_row_hash\")\n","            # n_new: claves en origen que no están en destino\n","            existing_keys = dst_df.select(*key_cols).distinct()\n","            n_new = df.join(existing_keys, on=key_cols, how=\"left_anti\").count()\n","            # n_changed: claves comunes con hash distinto\n","            joined = df_h.alias(\"s\").join(dst_h.alias(\"t\"), on=key_cols, how=\"inner\")\n","            n_changed = joined.filter(col(\"s._row_hash\") != col(\"t._row_hash\")).count()\n","        except Exception as e:\n","            print(f\"[WARN] No se pudo calcular n_new/n_changed para {dst_full}: {e}\")\n","            n_new = None\n","            n_changed = None\n","    else:\n","        # Si destino no existe, todo es nuevo\n","        if not dst_exists:\n","            n_new = n_total\n","            n_changed = 0\n","\n","    print(f\"[INFO] Filas en batch: {n_total}; estimado nuevas={n_new}; estimado cambios_reales={n_changed}\")\n","\n","    # Ejecutar MERGE condicional: actualizar solo si hay diferencias reales\n","    try:\n","        # Si la tabla no existe, crearla con append\n","        try:\n","            DeltaTable.forName(spark, dst_full)\n","            table_exists = True\n","        except Exception:\n","            table_exists = False\n","\n","        if not table_exists:\n","            df.write.format(\"delta\").mode(\"append\").saveAsTable(dst_full)\n","            print(f\"[OK] Tabla {dst_full} creada y datos escritos (primer load). Filas: {n_total}\")\n","            continue\n","\n","        # Verificar key_cols\n","        if not key_cols:\n","            raise ValueError(f\"No hay key_cols definidas para MERGE en {tbl}.\")\n","\n","        delta_tbl = DeltaTable.forName(spark, dst_full)\n","        join_cond = \" AND \".join([f\"t.{k}=s.{k}\" for k in key_cols])\n","\n","        # Si no hay columnas para comparar (solo ingestion_ts), usamos whenMatchedUpdateAll()\n","        if not compare_cols:\n","            (delta_tbl.alias(\"t\")\n","              .merge(df.alias(\"s\"), join_cond)\n","              .whenMatchedUpdateAll()\n","              .whenNotMatchedInsertAll()\n","              .execute()\n","            )\n","            print(f\"[OK] MERGE ejecutado en {dst_full} (no había columnas para comparar).\")\n","        else:\n","            # Construir condición SQL que detecta diferencias reales\n","            cond_sql = build_diff_condition_sql(compare_cols)\n","            # Preparar mapping set para la actualización (actualizar todas las columnas del DF)\n","            set_map = {c: f\"s.`{c}`\" for c in df.columns}\n","            # Ejecutar MERGE condicional: update solo si cond_sql es True\n","            (delta_tbl.alias(\"t\")\n","              .merge(df.alias(\"s\"), join_cond)\n","              .whenMatchedUpdate(condition=cond_sql, set=set_map)\n","              .whenNotMatchedInsertAll()\n","              .execute()\n","            )\n","            print(f\"[OK] MERGE condicional ejecutado en {dst_full}. Nuevas={n_new}, cambios_reales={n_changed}\")\n","\n","    except Exception as e:\n","        print(f\"[ERROR] MERGE falló para {dst_full}: {e}\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2025-12-29T07:44:37.1918778Z","session_start_time":"2025-12-29T07:44:37.1931494Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"39415a39-5d3e-4cec-be54-1c180e0b3de4"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2cff28d0-550e-423c-bb2b-4caf5c906a5a"},{"cell_type":"code","source":["from pyspark.sql.functions import (\n","    current_timestamp, lit, sha2, concat_ws, col, to_date, date_format, regexp_replace\n",")\n","from pyspark.sql.utils import AnalysisException\n","from delta.tables import DeltaTable\n","import uuid, datetime, os\n","\n","# Configuración\n","landing_path = \"Files/Ventas/\"            # <-- debe ser la carpeta que contiene los CSV\n","dst_table = \"Bronze.Ventas\"\n","audit_table = \"Auditoria.audit_venta\"\n","key_cols = [\"VentaID\"]\n","date_col = \"FechaVenta\"\n","\n","# Helper: listar archivos .csv (soporta Databricks dbutils, Hadoop FS o local)\n","def list_csv_files(path):\n","    try:\n","        files = [f.name for f in dbutils.fs.ls(path) if f.name.endswith(\".csv\")]\n","        return sorted(files)\n","    except NameError:\n","        pass\n","    except Exception:\n","        pass\n","    try:\n","        hadoop_conf = spark._jsc.hadoopConfiguration()\n","        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n","        path_obj = spark._jvm.org.apache.hadoop.fs.Path(path)\n","        if not fs.exists(path_obj):\n","            return []\n","        status = fs.listStatus(path_obj)\n","        files = []\n","        for s in status:\n","            if s.isFile():\n","                name = s.getPath().getName()\n","                if name.endswith(\".csv\"):\n","                    files.append(name)\n","        return sorted(files)\n","    except Exception:\n","        pass\n","    try:\n","        local_path = path.replace(\"file://\", \"\")\n","        files = [f for f in os.listdir(local_path) if f.endswith(\".csv\")]\n","        return sorted(files)\n","    except Exception:\n","        pass\n","    print(f\"[WARN] No se pudo listar archivos en {path}\")\n","    return []\n","\n","# Asegurar esquema Auditoria y tabla audit_venta\n","spark.sql(\"CREATE DATABASE IF NOT EXISTS Auditoria\")\n","spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS {audit_table} (\n","  filename STRING,\n","  file_path STRING,\n","  batch_id STRING,\n","  ingestion_ts TIMESTAMP,\n","  year_month STRING,\n","  row_count LONG,\n","  new_count LONG,\n","  updated_count LONG,\n","  status STRING,\n","  error_message STRING\n",") USING DELTA\n","\"\"\")\n","\n","# Obtener archivos nuevos\n","files = list_csv_files(landing_path)\n","processed = []\n","if spark._jsparkSession.catalog().tableExists(audit_table):\n","    processed = [r.filename for r in spark.table(audit_table).select(\"filename\").distinct().collect()]\n","new_files = sorted([f for f in files if f not in processed])\n","\n","if not new_files:\n","    print(\"No hay archivos nuevos para procesar.\")\n","else:\n","    print(\"Archivos nuevos:\", new_files)\n","\n","for fname in new_files:\n","    path = landing_path + fname\n","    batch_id = str(uuid.uuid4())\n","    ingestion_ts = datetime.datetime.now()\n","    print(f\"\\nProcesando archivo: {fname} (batch_id={batch_id})\")\n","\n","    try:\n","        # ---------- LECTURA COMO STRING (evita inferSchema problemático) ----------\n","        df_raw = (spark.read\n","                  .option(\"header\", \"true\")\n","                  .option(\"inferSchema\", \"false\")   # leer todo como string\n","                  .option(\"encoding\", \"utf-8\")\n","                  .csv(path)\n","                 )\n","\n","        # ---------- NORMALIZACIÓN BÁSICA ----------\n","        # Reemplazar valores comunes que rompen el casteo\n","        df_norm = df_raw.replace([\"N/A\", \"NA\", \"-\", \"—\", \"\"], [None, None, None, None, None])\n","\n","        # Limpiar separadores de miles/coma decimal si aplica (ejemplo)\n","        # Aplica regexp_replace solo a columnas numéricas que vienen como string\n","        if \"PrecioUnitario\" in df_norm.columns:\n","            df_norm = df_norm.withColumn(\"PrecioUnitario\", regexp_replace(col(\"PrecioUnitario\"), r\"[\\\\s]\", \"\"))\n","            df_norm = df_norm.withColumn(\"PrecioUnitario\", regexp_replace(col(\"PrecioUnitario\"), \",\", \".\"))\n","\n","        # ---------- CASTEO EXPLÍCITO Y VALIDADO ----------\n","        # Castear columnas críticas con control\n","        df = (df_norm\n","              .withColumn(\"VentaID\", col(\"VentaID\").cast(\"long\"))\n","              .withColumn(date_col, to_date(col(date_col), \"yyyy-MM-dd\"))\n","              .withColumn(\"ProductoID\", col(\"ProductoID\").cast(\"long\")) if \"ProductoID\" in df_norm.columns else df_norm\n","             )\n","\n","        # Si la línea anterior devolvió un DataFrame distinto, reasignar df correctamente:\n","        if isinstance(df, type(df_norm)):  # si no se aplicó ProductoID, df es df_norm\n","            df = df_norm.withColumn(\"VentaID\", col(\"VentaID\").cast(\"long\")).withColumn(date_col, to_date(col(date_col), \"yyyy-MM-dd\"))\n","        else:\n","            # ya tiene VentaID y FechaVenta casteados; aseguramos columnas adicionales si existen\n","            df = df.withColumn(\"VentaID\", col(\"VentaID\").cast(\"long\")).withColumn(date_col, to_date(col(date_col), \"yyyy-MM-dd\"))\n","\n","        # Añadir ingestion_ts, batch_id y year_month\n","        df = (df.withColumn(\"ingestion_ts\", current_timestamp())\n","                .withColumn(\"batch_id\", lit(batch_id))\n","                .withColumn(\"year_month\", date_format(col(date_col), \"yyyy-MM\"))\n","               )\n","\n","        # Deduplicate por clave dentro del batch\n","        df = df.dropDuplicates(key_cols).cache()\n","\n","        # Validaciones rápidas: detectar casts fallidos en columnas críticas\n","        bad_ventaid = df.filter(col(\"VentaID\").isNull()).count()\n","        bad_fecha = df.filter(col(date_col).isNull()).count()\n","        if bad_ventaid > 0 or bad_fecha > 0:\n","            raise ValueError(f\"Casteo fallido: VentaID null={bad_ventaid}, {date_col} null={bad_fecha}. Revisa formato del CSV.\")\n","\n","        row_count = df.count()\n","        year_months = [r[\"year_month\"] for r in df.select(\"year_month\").distinct().collect()]\n","        print(f\"Filas leídas: {row_count}; particiones en batch: {year_months}\")\n","\n","        # ---------- EXISTENCIA DESTINO ----------\n","        try:\n","            DeltaTable.forName(spark, dst_table)\n","            dst_exists = True\n","        except Exception:\n","            dst_exists = False\n","\n","        total_new = 0\n","        total_updated = 0\n","        status = \"processed\"\n","        error_message = None\n","\n","        if not dst_exists:\n","            # Primer load: crear tabla destino (particionar por year_month si quieres)\n","            df.write.format(\"delta\").mode(\"append\").saveAsTable(dst_table)\n","            total_new = row_count\n","            print(f\"Tabla {dst_table} creada con {row_count} filas.\")\n","        else:\n","            # Procesar por partición year_month para limitar scope del MERGE\n","            for ym in year_months:\n","                df_part = df.filter(col(\"year_month\") == ym).cache()\n","                compare_cols = [c for c in df_part.columns if c not in (\"ingestion_ts\",\"batch_id\",\"year_month\")]\n","\n","                # Si no hay columnas para comparar (raro), saltar MERGE condicional\n","                if not compare_cols:\n","                    print(f\"[WARN] No hay columnas para comparar en partition {ym}; se hará MERGE simple.\")\n","                    delta_tbl = DeltaTable.forName(spark, dst_table)\n","                    join_cond = \" AND \".join([f\"t.{k}=s.{k}\" for k in key_cols])\n","                    (delta_tbl.alias(\"t\")\n","                      .merge(df_part.alias(\"s\"), join_cond)\n","                      .whenMatchedUpdateAll()\n","                      .whenNotMatchedInsertAll()\n","                      .execute()\n","                    )\n","                    df_part.unpersist()\n","                    continue\n","\n","                # Preparar hashes para conteos precisos\n","                df_h = df_part.select(*key_cols, *compare_cols).withColumn(\"_row_hash\", sha2(concat_ws(\"||\", *compare_cols), 256))\n","                dst_part = spark.table(dst_table).filter(col(\"year_month\") == ym).select(*key_cols, *compare_cols)\n","                dst_h = dst_part.withColumn(\"_row_hash\", sha2(concat_ws(\"||\", *compare_cols), 256)).select(*key_cols, \"_row_hash\")\n","\n","                existing_keys = dst_part.select(*key_cols).distinct()\n","                n_new_part = df_part.join(existing_keys, on=key_cols, how=\"left_anti\").count()\n","                joined = df_h.alias(\"s\").join(dst_h.alias(\"t\"), on=key_cols, how=\"inner\")\n","                n_changed_part = joined.filter(col(\"s._row_hash\") != col(\"t._row_hash\")).count()\n","\n","                print(f\"Partition {ym}: filas_batch={df_part.count()}, nuevas={n_new_part}, cambios_reales={n_changed_part}\")\n","\n","                # Ejecutar MERGE condicional limitado a la partición\n","                delta_tbl = DeltaTable.forName(spark, dst_table)\n","                join_cond = \" AND \".join([f\"t.{k}=s.{k}\" for k in key_cols])\n","                conds = \" OR \".join([f\"(t.`{c}` <> s.`{c}` OR (t.`{c}` IS NULL AND s.`{c}` IS NOT NULL) OR (t.`{c}` IS NOT NULL AND s.`{c}` IS NULL))\" for c in compare_cols])\n","                set_map = {c: f\"s.`{c}`\" for c in df_part.columns}\n","\n","                (delta_tbl.alias(\"t\")\n","                  .merge(df_part.alias(\"s\"), join_cond)\n","                  .whenMatchedUpdate(condition=conds, set=set_map)\n","                  .whenNotMatchedInsertAll()\n","                  .execute()\n","                )\n","\n","                total_new += n_new_part\n","                total_updated += n_changed_part\n","                df_part.unpersist()\n","\n","        # Registrar resultado en Auditoria.audit_venta\n","        audit_row = [(fname, path, batch_id, ingestion_ts, \",\".join(year_months), row_count, total_new, total_updated, status, error_message)]\n","        spark.createDataFrame(audit_row, schema=[\"filename\",\"file_path\",\"batch_id\",\"ingestion_ts\",\"year_month\",\"row_count\",\"new_count\",\"updated_count\",\"status\",\"error_message\"]) \\\n","             .write.format(\"delta\").mode(\"append\").saveAsTable(audit_table)\n","\n","        print(f\"Archivo {fname} procesado. nuevas={total_new}, actualizadas={total_updated}\")\n","\n","    except Exception as e:\n","        status = \"error\"\n","        error_message = str(e)\n","        print(f\"[ERROR] al procesar {fname}: {error_message}\")\n","        # registrar error en auditoría (intenta incluir year_months si existe)\n","        ym_val = \",\".join(year_months) if 'year_months' in locals() else None\n","        audit_row = [(fname, path, batch_id, ingestion_ts, ym_val, 0, 0, 0, status, error_message)]\n","        spark.createDataFrame(audit_row, schema=[\"filename\",\"file_path\",\"batch_id\",\"ingestion_ts\",\"year_month\",\"row_count\",\"new_count\",\"updated_count\",\"status\",\"error_message\"]) \\\n","             .write.format(\"delta\").mode(\"append\").saveAsTable(audit_table)\n","\n","    finally:\n","        try:\n","            df.unpersist()\n","        except Exception:\n","            pass\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"4522f64b-382b-42bc-a437-94239f739695","normalized_state":"finished","queued_time":"2025-12-26T01:13:05.399168Z","session_start_time":null,"execution_start_time":"2025-12-26T01:13:05.4006667Z","execution_finish_time":"2025-12-26T01:13:35.0182591Z","parent_msg_id":"7a36433c-6c83-4560-84de-6426ea5ecf5a"},"text/plain":"StatementMeta(, 4522f64b-382b-42bc-a437-94239f739695, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Archivos nuevos: ['ventas_2024-09.csv']\n\nProcesando archivo: ventas_2024-09.csv (batch_id=5d69b53b-f49e-4031-9779-87396149219b)\nFilas leídas: 10; particiones en batch: ['2024-09']\nPartition 2024-09: filas_batch=10, nuevas=10, cambios_reales=0\n[ERROR] al procesar ventas_2024-09.csv: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f4fd58c-53e9-4201-b824-101e7a3221b2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"es"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"4ace26ab-f67b-4c45-9cf0-37d8b89a078e"}],"default_lakehouse":"4ace26ab-f67b-4c45-9cf0-37d8b89a078e","default_lakehouse_name":"Landing","default_lakehouse_workspace_id":"acb6da67-79e7-44c7-8925-ba025a0dd6c0"}}},"nbformat":4,"nbformat_minor":5}